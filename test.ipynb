{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c077be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import LLM, ImageGen \n",
    "from core import get_image_models , get_llm_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a28e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['deepseek', 'deepseek-llama-3.3-70b', 'deepseek-r-1', 'deepseek-r-1-turbo', 'deepseek-r-1-zero', 'deepseek-v-2', 'deepseek-v-2.5', 'deepseek-v-3', 'gemini', 'gemini-1.5-flash', 'gemini-1.5-pro', 'gemini-2-flash', 'gemini-2-flash-thinking', 'gemini-2-flash-thinkingerimental', 'gemini-2.5-flash-lite-thinking', 'gpt-3.5-turbo', 'gpt-4', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4o', 'gpt-5', 'grok-3', 'o-1', 'o-3', 'o-4-mini', 'qwen-2.5-72b-instruct', 'qwen-2.5-coder-32b-instruct', 'qwen-2.5-plus', 'qwen-2.5-vl-32b-instruct', 'qwen-max', 'qwen-plus', 'qwen-turbo'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_llm_models().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb1d6a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying model: DeepSeek-R1, provider: Chatai\n",
      "Response received:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LLM(model_name='deepseek-r-1')\n",
    "model.chat(\"hi how are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a3fe72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import LLM, ImageGen \n",
    "from core import get_image_models , get_llm_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65592f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import asyncio\n",
    "# from g4f.client import Client\n",
    "\n",
    "# client = Client()\n",
    "\n",
    "# async def generate_image():\n",
    "#     response = await client.images.async_generate(\n",
    "#         model=\"flux\", \n",
    "#         prompt=\"a white siamese cat\", \n",
    "#         response_format=\"url\"\n",
    "#     )\n",
    "#     print(f\"Generated image URL: {response.data[0].url}\")\n",
    "\n",
    "# await generate_image()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bff7e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['anima-pencil-xl', 'flux', 'flux-1-kontext-dev', 'flux-1-kontext-max', 'flux-1-kontext-pro', 'flux-1.1-pro', 'flux-canny', 'flux-depth', 'flux-dev', 'flux-dev-lora', 'flux-kontext', 'flux-kontext-dev', 'flux-kontext-pro', 'flux-krea-blaze', 'flux-krea-dev', 'flux-krea-dev-gguf', 'flux-pro', 'flux-redux', 'flux-schnell', 'imagen-3-generate', 'imagen-3-generate-002', 'imagen-4-generate', 'imagen-4-ultra-generate', 'sdxl-1', 'sdxl-l', 'sdxl-turbo', 'stable-diffusion-3.5-large', 'stable-diffusion-3.5-small-1', 'stable-diffusion-v-1-5', 'stable-diffusion-xl-base-1'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_image_models().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e4e447f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating image for prompt: 'A photorealistic portrait of an astronaut on a horse'...\n",
      "Trying model variation: PollinationsAI:flux...\n",
      "Image generated successfully!\n"
     ]
    }
   ],
   "source": [
    "image_generator = ImageGen(model_name='flux')\n",
    "\n",
    "prompt = \"A photorealistic portrait of an astronaut on a horse\"\n",
    "print(f\"Generating image for prompt: '{prompt}'...\")\n",
    "response = await image_generator.generate(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aa1df6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://image.pollinations.ai/prompt/A+photorealistic+portrait+of+an+astronaut+on+a+horse?width=1024&height=1024&model=PollinationsAI:flux&nologo=true&private=false&enhance=false&safe=false&referrer=https://g4f.dev/&seed=1587485426'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab022658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/1755395749_A_photorealistic_portrait_of_an_astronaut_on_a_horse_f779be02f10971b2.jpg'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a669d5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/tmp/gradio': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "from core import ImageGen\n",
    "# 1. Initialize the ImageGen class with a model name\n",
    "#    (You can get model names by running `main.py --list_models`)\n",
    "try:\n",
    "    image_generator = ImageGen(model_name=\"stable-diffusion-xl-base-1\")\n",
    "    # 2. Define your prompt\n",
    "    prompt = \"A photorealistic portrait of an astronaut on a horse\"\n",
    "    # 3. Call the .generate() method\n",
    "    #    This will automatically find a working provider and generate the image.\n",
    "    print(f\"Generating image for prompt: '{prompt}'...\")\n",
    "    response = image_generator.generate(prompt)\n",
    "    # 4. Print the response\n",
    "    if response:\n",
    "        print(\"\\\\nImage generated successfully!\")\n",
    "        print(f\"Response: {response}\")\n",
    "except Exception as e:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f64ead8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deepseek': ['deepseek', 'deepseek-chat', 'deepseek-distill-qwen-32b'],\n",
       " 'deepseek-llama-3.3-70b': ['deepseek-llama-3.3-70b', 'deepseek-llama3.3-70b'],\n",
       " 'deepseek-r-1': ['DeepSeek-R1',\n",
       "  'deepseek-r1',\n",
       "  'deepseek-r1-0528',\n",
       "  'deepseek-r1-distill-llama-70b',\n",
       "  'deepseek-r1-distill-llama-8b',\n",
       "  'deepseek-r1-distill-qwen-1.5b',\n",
       "  'deepseek-r1-distill-qwen-14b',\n",
       "  'deepseek-r1-distill-qwen-32b'],\n",
       " 'deepseek-r-1-turbo': ['deepseek-r1-0528-turbo', 'deepseek-r1-turbo'],\n",
       " 'deepseek-r-1-zero': ['DeepSeek R1 Zero', 'deepseek-r1-zero'],\n",
       " 'deepseek-v-2': ['deepseek-v2', 'deepseek-v2-api-0628'],\n",
       " 'deepseek-v-2.5': ['deepseek-v2.5', 'deepseek-v2.5-1210'],\n",
       " 'deepseek-v-3': ['DeepSeek V3',\n",
       "  'DeepSeek V3 0324',\n",
       "  'DeepSeek-V3',\n",
       "  'deepseek-chat-v3-0324',\n",
       "  'deepseek-v3',\n",
       "  'deepseek-v3-0324',\n",
       "  'hf:deepseek-ai/DeepSeek-V3'],\n",
       " 'gemini': ['gemini-exp-1114', 'gemini-exp-1121', 'gemini-exp-1206'],\n",
       " 'gemini-1.5-flash': ['gemini-1.5-flash', 'gemini-1.5-flash-exp-0827'],\n",
       " 'gemini-1.5-pro': ['gemini-1.5-pro',\n",
       "  'gemini-1.5-pro-api-0409',\n",
       "  'gemini-1.5-pro-exp-0801',\n",
       "  'gemini-1.5-pro-exp-0827'],\n",
       " 'gemini-2-flash': ['gemini-2.0-flash', 'gemini-2.0-flash-exp'],\n",
       " 'gemini-2-flash-thinking': ['gemini-2.0-flash-thinking',\n",
       "  'gemini-2.0-flash-thinking-exp',\n",
       "  'gemini-2.0-flash-thinking-exp-1219',\n",
       "  'gemini-2.0-flash-thinking-with-apps'],\n",
       " 'gemini-2-flash-thinkingerimental': ['Gemini 2.0 Flash Thinking Experimental',\n",
       "  'Gemini 2.0 Flash Thinking Experimental 01-21'],\n",
       " 'gemini-2.5-flash-lite-thinking': ['gemini-2.5-flash-lite-preview-06-17-thinking',\n",
       "  'gemini-2.5-flash-lite-preview-thinking'],\n",
       " 'gpt-3.5-turbo': ['gpt-3.5-turbo',\n",
       "  'gpt-3.5-turbo-0125',\n",
       "  'gpt-3.5-turbo-0314',\n",
       "  'gpt-3.5-turbo-0613',\n",
       "  'gpt-3.5-turbo-1106'],\n",
       " 'gpt-4': ['gpt-4', 'gpt-4-0125', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-1106'],\n",
       " 'gpt-4.1': ['gpt-4.1', 'gpt-4.1-2025-04-14'],\n",
       " 'gpt-4.1-mini': ['gpt-4.1-mini', 'gpt-4.1-mini-2025-04-14'],\n",
       " 'gpt-4o': ['chatgpt-4o', 'chatgpt-4o-latest-20250326', 'gpt-4o', 'gpt4o'],\n",
       " 'gpt-5': ['gpt-5', 'gpt-5-chat'],\n",
       " 'grok-3': ['grok-3', 'grok-3-preview-02-24'],\n",
       " 'o-1': ['o1', 'o1-preview'],\n",
       " 'o-3': ['o3', 'o3-2025-04-16'],\n",
       " 'o-4-mini': ['o4-mini', 'o4-mini-2025-04-16'],\n",
       " 'qwen-2.5-72b-instruct': ['Qwen2.5 72B Instruct',\n",
       "  'hf:Qwen/Qwen2.5-72B-Instruct',\n",
       "  'qwen2.5-72b-instruct'],\n",
       " 'qwen-2.5-coder-32b-instruct': ['Qwen2.5 Coder 32B Instruct',\n",
       "  'hf:Qwen/Qwen2.5-Coder-32B-Instruct'],\n",
       " 'qwen-2.5-plus': ['qwen-2.5-plus', 'qwen-2.5-plus-1127'],\n",
       " 'qwen-2.5-vl-32b-instruct': ['Qwen2.5 VL 32B Instruct',\n",
       "  'qwen2.5-vl-32b-instruct'],\n",
       " 'qwen-max': ['qwen-max',\n",
       "  'qwen-max-0428',\n",
       "  'qwen-max-0919',\n",
       "  'qwen-max-2025-01-25',\n",
       "  'qwen-max-latest'],\n",
       " 'qwen-plus': ['qwen-plus',\n",
       "  'qwen-plus-0125',\n",
       "  'qwen-plus-0828',\n",
       "  'qwen-plus-2025-01-25'],\n",
       " 'qwen-turbo': ['qwen-turbo', 'qwen-turbo-2025-02-11']}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scripts.get_llm_models import get_llm_models\n",
    "\n",
    "get_llm_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a7985bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anima-pencil-xl': ['anima-pencil-xl'],\n",
       " 'flux': ['PollinationsAI:flux', 'flux'],\n",
       " 'flux-1-kontext-dev': ['flux-1-kontext-dev'],\n",
       " 'flux-1-kontext-max': ['flux-1-kontext-max'],\n",
       " 'flux-1-kontext-pro': ['flux-1-kontext-pro'],\n",
       " 'flux-1.1-pro': ['flux-1.1-pro'],\n",
       " 'flux-canny': ['flux-canny'],\n",
       " 'flux-depth': ['flux-depth'],\n",
       " 'flux-dev': ['flux-dev'],\n",
       " 'flux-dev-lora': ['flux-dev-lora'],\n",
       " 'flux-kontext': ['flux-kontext'],\n",
       " 'flux-kontext-dev': ['flux-kontext-dev'],\n",
       " 'flux-kontext-pro': ['flux-kontext-pro'],\n",
       " 'flux-krea-blaze': ['flux-krea-blaze'],\n",
       " 'flux-krea-dev': ['flux-krea-dev'],\n",
       " 'flux-krea-dev-gguf': ['flux-krea-dev-gguf'],\n",
       " 'flux-pro': ['flux-pro'],\n",
       " 'flux-redux': ['flux-redux'],\n",
       " 'flux-schnell': ['flux-schnell'],\n",
       " 'imagen-3-generate': ['imagen-3.0-generate'],\n",
       " 'imagen-3-generate-002': ['imagen-3.0-generate-002'],\n",
       " 'imagen-4-generate': ['imagen-4.0-generate',\n",
       "  'imagen-4.0-generate-preview-06-06'],\n",
       " 'imagen-4-ultra-generate': ['imagen-4.0-ultra-generate',\n",
       "  'imagen-4.0-ultra-generate-preview-06-06'],\n",
       " 'sdxl-1': ['sdxl-1.0'],\n",
       " 'sdxl-l': ['sdxl-l'],\n",
       " 'sdxl-turbo': ['sdxl-turbo'],\n",
       " 'stable-diffusion-3.5-large': ['stable-diffusion-3.5-large'],\n",
       " 'stable-diffusion-3.5-small-1': ['stable-diffusion-3.5-small-preview1'],\n",
       " 'stable-diffusion-v-1-5': ['stable-diffusion-v1-5'],\n",
       " 'stable-diffusion-xl-base-1': ['stable-diffusion-xl-base-1.0']}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scripts.get_image_models import get_image_models\n",
    "\n",
    "get_image_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30509bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created models.json with filtered and grouped model names.\n"
     ]
    }
   ],
   "source": [
    "import g4f\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def normalize_model_name(name):\n",
    "    \"\"\"\n",
    "    Normalizes a model name to group similar models.\n",
    "    \"\"\"\n",
    "    s = name.lower()\n",
    "    s = s.replace('_', '-').replace(' ', '-')\n",
    "\n",
    "    # Remove common prefixes like 'hf:', 'meta-llama/', etc.\n",
    "    if ':' in s:\n",
    "        s = s.split(':', 1)[-1]\n",
    "    if '/' in s:\n",
    "        s = s.split('/', 1)[-1]\n",
    "\n",
    "    s = s.replace('_', '-')\n",
    "    s = re.sub(r'([a-zA-Z])(\\d)', r'\\1-\\2', s)  # insert hyphen between letters+numbers\n",
    "    s = re.sub(r'(\\d+)\\.0\\b', r'\\1', s)         # remove .0 from version numbers\n",
    "\n",
    "    # # Remove date-like patterns\n",
    "    # s = re.sub(r'-\\d{2}-\\d{2}', '', s)  # e.g., -06-17\n",
    "    # s = re.sub(r'\\b\\d{4}\\b', '', s)     # e.g., 2024\n",
    "    # s = re.sub(r'\\b\\d{6}\\b', '', s)     # e.g., 202401\n",
    "    # s = re.sub(r'\\b\\d{8}\\b', '', s)     # e.g., 20240101\n",
    "\n",
    "    # remove date-like patterns\n",
    "    s = re.sub(r'[- ]\\d{2}[- ]\\d{2}', '', s)  # handles -06-17 or 06-17 with spaces\n",
    "    s = re.sub(r'\\b\\d{4}\\b', '', s)           # 2024\n",
    "    s = re.sub(r'\\b\\d{6}\\b', '', s)           # 202401\n",
    "    s = re.sub(r'\\b\\d{8}\\b', '', s)           # 20240101\n",
    "\n",
    "    # Truncate after 'distill'\n",
    "    distill_match = re.search(r'distill', s)\n",
    "    if distill_match:\n",
    "        s = s[:distill_match.end()]\n",
    "\n",
    "    # Remove common suffixes\n",
    "    s = re.sub(r'-latest|-exp|-experimental|-with-apps|-preview|-distill', '', s)\n",
    "    s = re.sub(r'api|lora|image|audio|chat|experimental', '', s)\n",
    "\n",
    "    # Cleanup: collapse multiple hyphens and strip leading/trailing\n",
    "    s = re.sub(r'-+', '-', s)       # collapse repeated hyphens\n",
    "    s = s.strip('- ')\n",
    "\n",
    "    return s\n",
    "\n",
    "def create_model_json():\n",
    "    \"\"\"\n",
    "    Gathers all models from g4f providers, groups them by a normalized name,\n",
    "    filters out smaller models, and saves the result to a JSON file.\n",
    "    \"\"\"\n",
    "    model_groups = defaultdict(set)\n",
    "\n",
    "    for provider in g4f.Provider.__providers__:\n",
    "        if hasattr(provider, 'models') and provider.models:\n",
    "            model_list = []\n",
    "            if isinstance(provider.models, list):\n",
    "                model_list = provider.models\n",
    "            elif isinstance(provider.models, dict):\n",
    "                model_list = list(provider.models.keys())\n",
    "            elif isinstance(provider.models, set):\n",
    "                model_list = list(provider.models)\n",
    "\n",
    "            for model_name in model_list:\n",
    "                if isinstance(model_name, str) and model_name:\n",
    "                    normalized_name = normalize_model_name(model_name)\n",
    "\n",
    "                    if normalized_name:\n",
    "                        # Skip audio/image/coder/tts models\n",
    "                        if any(x in model_name for x in [\"audio\", \"image\", \"tts\", \"coder\", \"ghibli\"]):\n",
    "                            continue\n",
    "                        # Skip non-ascii names\n",
    "                        try:\n",
    "                            normalized_name.encode('ascii')\n",
    "                        except UnicodeEncodeError:\n",
    "                            continue\n",
    "                        model_groups[normalized_name].add(model_name)\n",
    "\n",
    "    # Filter out smaller models\n",
    "    filtered_model_groups = {}\n",
    "    for name, raw_names in model_groups.items():\n",
    "        keep_model = True\n",
    "        matches = re.finditer(r'(\\d+(?:\\.\\d+)?)([bm])', name)\n",
    "        for match in matches:\n",
    "            try:\n",
    "                number = float(match.group(1))\n",
    "                if number < 30:\n",
    "                    preceding_text = name[:match.start()]\n",
    "                    words_before = preceding_text.split()\n",
    "                    if not (words_before and words_before[-1].endswith('distilled')):\n",
    "                        keep_model = False\n",
    "                        break\n",
    "            except ValueError:\n",
    "                pass\n",
    "        if keep_model:\n",
    "            filtered_model_groups[name] = raw_names\n",
    "\n",
    "    # Famous LLMs we keep\n",
    "    # famous_llm_names = [\"gemini\", \"gpt\", \"anthropic\", \"grok\", \"mistral\", \"llama\", \"deepseek\", \"o-\", \"qwen\"]\n",
    "    famous_llm_names = [\"gemini\", \"gpt\", \"anthropic\", \"grok\", \"deepseek\", \"o-\", \"qwen\"]\n",
    "\n",
    "    final_filtered_model_groups = {}\n",
    "    for name, raw_names in filtered_model_groups.items():\n",
    "        for llm_name in famous_llm_names:\n",
    "            if name.startswith(llm_name):\n",
    "                final_filtered_model_groups[name] = raw_names\n",
    "                break\n",
    "\n",
    "    # Convert sets to sorted lists\n",
    "    final_model_groups = {\n",
    "        name: sorted(list(raw_names))\n",
    "        for name, raw_names in final_filtered_model_groups.items()\n",
    "    }\n",
    "\n",
    "    # Sort keys\n",
    "    sorted_model_groups = dict(sorted(final_model_groups.items()))\n",
    "\n",
    "    with open('models.json', 'w') as f:\n",
    "        json.dump(sorted_model_groups, f, indent=2)\n",
    "\n",
    "    print(\"Successfully created models.json with filtered and grouped model names.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_model_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb0397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "g4f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
